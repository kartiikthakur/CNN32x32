{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as py\n",
    "\n",
    "\n",
    "DATADIR= \"C:/Semester 2/ML/Project/food/food-101/images\"\n",
    "#CATEGORIES=[\"french_fries\",\"fried_rice\"]\n",
    "CATEGORIES = [\"chicken_curry\", \"chicken_wings\", \"cup_cakes\", \"donuts\",\"french_fries\",\"fried_rice\"\n",
    "             ,\"omelette\",\"pizza\",\"samosa\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[]\n",
    "\n",
    "def create_training_data():\n",
    "    IMG_SIZE=255\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array,class_num])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)\n",
    "IMG_SIZE = 255\n",
    "X=[]\n",
    "y=[]\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X,pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-conv-64-nodes-1-dense-1557041481\n",
      "Train on 6300 samples, validate on 2700 samples\n",
      "6300/6300 [==============================] - 2029s 322ms/sample - loss: 10.3569 - categorical_accuracy: 0.0000e+00 - val_loss: 10.2692 - val_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# more info on callbakcs: https://keras.io/callbacks/ model saver is cool too.\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "\n",
    "dense_layers = [1]\n",
    "layer_sizes = [64]\n",
    "conv_layers = [3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(NAME)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(10))\n",
    "            model.add(Activation('relu'))\n",
    "\n",
    "            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "            model.compile(loss='sparse_categorical_crossentropy',\n",
    "                          optimizer='rmsprop',\n",
    "                          metrics=['categorical_accuracy'],\n",
    "                          )\n",
    "\n",
    "            model.fit(X, y,\n",
    "                      batch_size=128,\n",
    "                      epochs=1,\n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])\n",
    "\n",
    "model.save('FOOD10.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-690051378c42>, line 112)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-690051378c42>\"\u001b[1;36m, line \u001b[1;32m112\u001b[0m\n\u001b[1;33m    print \"Number of labels {}\".format(len(labels_list))\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import applications\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sets import Set\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def show_acc_history(history):\n",
    "    plt.clf()\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    if TYPE_CLASSIFIER is 'multiclass':\n",
    "        plt.plot(history.history['categorical_accuracy'])\n",
    "        plt.plot(history.history['val_categorical_accuracy'])\n",
    "    else:\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "    plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
    "    plt.savefig(os.path.join(HISTORY_DIR, 'acc_inceptionv3_' + TYPE_CLASSIFIER + '.png'))\n",
    "\n",
    "\n",
    "def show_loss_history(history):\n",
    "    plt.clf()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'test_loss'], loc='best')\n",
    "    plt.savefig(os.path.join(HISTORY_DIR, 'loss_inceptionv3_' + TYPE_CLASSIFIER + '.png'))\n",
    "\n",
    "\n",
    "def multilabel_flow_from_directory(flow_from_directory_gen):\n",
    "\n",
    "    while True: #keras needs infinite generators\n",
    "        x, y = next(flow_from_directory_gen)\n",
    "        classes = np.argmax(y, axis=1)\n",
    "        labels = []\n",
    "        for cl in classes:\n",
    "            labels.append(recipe_food_dict[label_map[cl].split(\"_\")[0]])\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        mlb = MultiLabelBinarizer(labels_list)\n",
    "        labels = mlb.fit_transform(labels)\n",
    "        yield x, labels\n",
    "\n",
    "\n",
    "def build_labels_dict(dataset_path, recipe_food_map_path):\n",
    "    print(\"[INFO] loading labels ...\")\n",
    "    recipe_food_map = np.genfromtxt(recipe_food_map_path, delimiter=\"\\t\", dtype=str)\n",
    "    recipe_label = np.genfromtxt(os.path.join(dataset_path, 'label.tsv'), delimiter=\"_\", dtype=str)\n",
    "    recipe_ids = recipe_label[:, 0].tolist()\n",
    "    recipe_food_dict = {}\n",
    "    labels_list = Set([])\n",
    "\n",
    "    for recipe_food in recipe_food_map:\n",
    "        if recipe_food[0] in recipe_food_dict:\n",
    "            if recipe_food[0] in recipe_ids:\n",
    "                recipe_food_dict[recipe_food[0]].append(recipe_food[2])\n",
    "                labels_list.add(recipe_food[2])\n",
    "        else:\n",
    "            if recipe_food[0] in recipe_ids:\n",
    "                recipe_food_dict[recipe_food[0]] = [recipe_food[2]]\n",
    "                labels_list.add(recipe_food[2])\n",
    "\n",
    "    labels_list = list(labels_list)\n",
    "    labels_list.sort()\n",
    "    return recipe_food_dict, labels_list\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    HISTORY_DIR = 'history'\n",
    "    MODELS_DIR = 'models'\n",
    "    DATA_DIR = '\"C:/Semester 2/ML/Project/test10\"'\n",
    "    RECIPE_FOOD_MAP = os.path.join(DATA_DIR, 'food_food_category_map.tsv')\n",
    "    TYPE_CLASSIFIER = 'multilabel' # accepted values only: ['multiclass', 'multilabel'] \n",
    "    TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "    VALID_DIR = os.path.join(DATA_DIR, 'valid')\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 100\n",
    "    INIT_LR = 1e-6\n",
    "    IMG_WIDTH, IMG_HEIGHT = 299, 299  # dimensions of our images\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, IMG_WIDTH, IMG_HEIGHT)\n",
    "    else:\n",
    "        input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "\n",
    "    num_train_samples = sum([len(files) for r, d, files in os.walk(TRAIN_DIR)])\n",
    "    num_valid_samples = sum([len(files) for r, d, files in os.walk(VALID_DIR)])\n",
    "\n",
    "    num_train_steps = num_train_samples // BATCH_SIZE + 1\n",
    "    num_valid_steps = num_valid_samples // BATCH_SIZE + 1\n",
    "\n",
    "    recipe_food_dict, labels_list = build_labels_dict(DATA_DIR, RECIPE_FOOD_MAP)\n",
    "    print \"Number of labels {}\".format(len(labels_list))\n",
    "\n",
    "    # construct the image generator for data augmentation\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                       rotation_range=25,\n",
    "                                       width_shift_range=0.1,\n",
    "                                       height_shift_range=0.1,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True,\n",
    "                                       fill_mode=\"nearest\")\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    train_generator = train_datagen.flow_from_directory(TRAIN_DIR, target_size=(IMG_WIDTH, IMG_HEIGHT), batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "    validation_generator = test_datagen.flow_from_directory(VALID_DIR, target_size=(IMG_WIDTH, IMG_HEIGHT), batch_size=BATCH_SIZE, class_mode='categorical')\n",
    "\n",
    "    label_map = (train_generator.class_indices)\n",
    "    label_map = dict((v, k) for k, v in label_map.items())\n",
    "    if TYPE_CLASSIFIER is 'multilabel':\n",
    "        multilabel_train_generator = multilabel_flow_from_directory(train_generator)\n",
    "        multilabel_validation_generator = multilabel_flow_from_directory(validation_generator)\n",
    "\n",
    "    # create the base pre-trained model\n",
    "    base_model = applications.inception_v3.InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # and a logistic layer\n",
    "    if TYPE_CLASSIFIER is 'multiclass':\n",
    "        predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "    else:\n",
    "        predictions = Dense(len(labels_list), activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # compile the model using binary cross-entropy rather than categorical cross-entropy -- this may seem counterintuitive for\n",
    "    # multi-label classification, but keep in mind that the goal here is to treat each output label as an independent Bernoulli distribution\n",
    "    if TYPE_CLASSIFIER is 'multiclass':\n",
    "        model.compile(optimizer=optimizers.Adam(lr=INIT_LR), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    else:\n",
    "        model.compile(optimizer=optimizers.Adam(lr=INIT_LR), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(patience=15)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(os.path.join(MODELS_DIR, 'inceptionv3_' + TYPE_CLASSIFIER + '_best.h5'), verbose=1, save_best_only=True)\n",
    "\n",
    "    # train the network\n",
    "    print(\"[INFO] training network...\")\n",
    "\n",
    "    if TYPE_CLASSIFIER is 'multiclass':\n",
    "        history = model.fit_generator(train_generator, steps_per_epoch=num_train_steps, epochs=EPOCHS, verbose=1, callbacks=[early_stopping, checkpointer], validation_data=validation_generator, validation_steps=num_valid_steps, workers=12, use_multiprocessing=True)\n",
    "    else:\n",
    "        history = model.fit_generator(multilabel_train_generator, steps_per_epoch=num_train_steps, epochs=EPOCHS, verbose=1, callbacks=[early_stopping, checkpointer], validation_data=multilabel_validation_generator, validation_steps=num_valid_steps, workers=12, use_multiprocessing=True)\n",
    "    model.save(os.path.join(MODELS_DIR, 'inceptionv3_' + TYPE_CLASSIFIER + '_final.h5'))\n",
    "    show_acc_history(history)\n",
    "    show_loss_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
